---
title: "Variance Regularization: Part Two"
author: "Jason Willwerscheid"
date: "3/11/2020"
output:
  workflowr::wflow_html:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

In a [previous analysis](var_reg_pbmc.html), I put a prior on the residual variance parameters to prevent them from going to zero during the backfit (in the past, I had just been thresholding them). I fixed the prior rather than estimating it using empirical Bayes: as it turns out, the latter is simply not effective (I tried a range of prior families, including exponential and gamma priors).

Here, I combine the two approaches. I reason about Poisson mixtures to set a minimum threshold and then, after thresholding, I use empirical Bayes to shrink the variance estimates towards their mean.

As in the previous analysis, all fits add 20 factors greedily using point-normal priors and then backfit. Here, however, I don't "pre-scale" cells (that is, I scale using library size normalization, but I don't do any additional scaling based on cell-wise variance estimates). The code used to produce the fits can be viewed [here](https://github.com/willwerscheid/scFLASH/blob/master/code/var_reg/varreg_fits2.R).

## Residual variance threshold

The "true" distribution of gene $j$ can be modeled as
$$ X_{ij} \sim \text{Poisson}(s_i \lambda_{ij}), $$
where $s_i$ is the size factor for cell $i$ and $\lambda_{ij}$ depends on, for example, cell type. Using a Taylor approximation, the transformed entry $Y_{ij} = \log(X_{ij} / s_i + 1)$ can be written
$$ Y_{ij} \approx \log(\lambda_{ij} + 1) 
+ \frac{1}{s_i(\lambda_{ij} + 1)}(X_{ij} - s_i \lambda_{ij})
- \frac{1}{2s_i^2(\lambda_{ij} + 1)^2}(X_{ij} - s_i \lambda_{ij})^2 $$
so that
$$ \mathbb{E}Y_{ij} \approx \log(\lambda_{ij} + 1) 
- \frac{\lambda_{ij}}{2s_i(\lambda_{ij} + 1)^2}$$
and
$$ \text{Var}(Y_{ij}) \approx \frac{\lambda_{ij}}{s_i(\lambda_{ij} + 1)^2}$$

The law of total variance gives a simple lower bound:
$$ \text{Var}(Y_j) \ge \mathbb{E}_i(\text{Var}(Y_{ij})) 
\approx \frac{1}{n} \sum_{i} \frac{\lambda_{ij}}{s_i(\lambda_{ij} + 1)^2} $$
Plugging in the estimator $\hat{\lambda}_{ij} = X_{ij} / s_i$:
$$ \text{Var}(Y_j) \ge \frac{1}{n} \sum_i \frac{X_{ij}}{s_i^2(\exp(Y_{ij}))^2} $$

Thus a reasonable lower bound for the residual variance estimates is
$$ \min_j \frac{1}{n} \sum_i \frac{X_{ij}}{s_i^2(\exp(Y_{ij}))^2} $$

## Prior family

I'll use the family of gamma priors, since they have the advantage of being fast (unlike gamma and exponential mixtures) and yet flexible (as compared to one-parameter exponential priors).

## Results: Variance Estimates

The regularization step doesn't seem to be hugely important. Variance estimates are very similar with and without it (the solid lines indicate the threshold):

```{r res1}
suppressMessages(library(tidyverse))
suppressMessages(library(Matrix))

source("./code/utils.R")
orig.data <- readRDS("./data/10x/pbmc.rds")
pbmc <- preprocess.pbmc(orig.data)

res <- readRDS("./output/var_reg/varreg_fits2.rds")

var_df <- tibble(thresholded = res$unreg$fl$residuals.sd^2,
                 regularized = res$reg$fl$residuals.sd^2)

ggplot(var_df, aes(x = thresholded, y = regularized)) + 
  geom_point(size = 1) +
  scale_x_log10() + 
  scale_y_log10() +
  geom_abline(slope = 1, linetype = "dashed") +
  ggtitle("Variance Estimates") +
  labs(x = "Without regularization", y = "With regularization") +
  geom_vline(xintercept = 1 / res$unreg$fl$flash.fit$given.tau) +
  geom_hline(yintercept = 1 / res$unreg$fl$flash.fit$given.tau)
```

The regularized fit is slower, but it does a bit better with respect to both the ELBO (surprisingly!) and the log likelihood of the implied discrete distribution.

```{r res2}
res_df <- tibble(Fit = c("Without Regularization", "With Regularization"),
                 ELBO = sapply(res, function(x) x$fl$elbo),
                 Discrete.Llik = sapply(res, function(x) x$p.vals$llik),
                 Elapsed.Time = sapply(res, function(x) x$elapsed.time))
knitr::kable(res_df, digits = 0)
```

## Results: Gene-wise thresholding

The argument I made above can in fact be applied gene by gene. That is, I can impose a gene-wise threshold rather than a single threshold for all genes: 
$$ \text{Var}(Y_j) \ge \frac{1}{n} \sum_i \frac{X_{ij}}{s_i^2(\exp(Y_{ij}))^2} $$

Indeed, I could go a bit further and estimate the sampling variance directly rather than calculating a rough lower bound:
$$ \text{Var}(Y_j) = \mathbb{E}_i(\text{Var}(Y_{ij})) + \text{Var}_i(\mathbb{E} Y_{ij})
\approx \frac{1}{n} \sum_{i} \frac{\lambda_{ij}}{s_i(\lambda_{ij} + 1)^2} 
+ \text{Var}_i \left( \log(\lambda_{ij} + 1) 
- \frac{\lambda_{ij}}{2s_i(\lambda_{ij} + 1)^2} \right) $$
The problem, however, is that if the "true" rate $\lambda_{ij}$ is the same for all $i$ (and, for the sake of argument, let all $s_i$ be identical), then $\text{Var}_i(\mathbb{E} Y_{ij})$ should be zero. If, however, the plug-in estimators $\hat{\lambda}_{ij} = X_{ij} / s_i$ are used, then $\text{Var}_i(\mathbb{E} Y_{ij})$ will be estimated as positive and the residual variance estimate for gene $j$ risks being too large. For this reason, I think that the best one can do is to use the rough lower bound and then let `flash` estimate the residual variance.

Note, however, that all but one `flash` estimate is already greater than this lower bound (for this reason, I won't bother to re-fit):

```{r gw.lb}
tmp.mat <- t(t(orig.data[-pbmc$dropped.genes, ] / (exp(pbmc$data))^2) / pbmc$size.factors^2)
var.lower.bd <- apply(tmp.mat, 1, mean)
var.est <- var.lower.bd + apply(pbmc$data - 0.5 * tmp.mat, 1, var)

var_df <- tibble(flash.estimate = res$reg$fl$residuals.sd^2,
                 var.lower.bd = var.lower.bd,
                 var.est = var.est)

ggplot(var_df, aes(x = var.lower.bd, y = flash.estimate)) + 
  geom_point(size = 1) +
  scale_x_log10() + 
  scale_y_log10() +
  geom_abline(slope = 1, linetype = "dashed")
```

In contrast, all of the `flash` estimates are lesser than the direct sampling variance estimate obtained using plug-in estimators, which strongly suggests that this approach would be inappropriate:

```{r var.est}
ggplot(var_df, aes(x = var.est, y = flash.estimate)) + 
  geom_point(size = 1) +
  scale_x_log10() + 
  scale_y_log10() +
  geom_abline(slope = 1, linetype = "dashed")
```

## Results: Factor comparisons

Factor plots are nearly identical with and without regularization. I like regularization on principle, but I'm not sure that it actually makes much of a difference.

```{r factors}
set.seed(666)
plot.factors(res$unreg, pbmc$cell.type, kset = order(res$unreg$fl$pve, decreasing = TRUE),
             title = "Without regularization")
set.seed(666)
plot.factors(res$reg, pbmc$cell.type, kset = order(res$reg$fl$pve, decreasing = TRUE), 
             title = "With regularization")
```
