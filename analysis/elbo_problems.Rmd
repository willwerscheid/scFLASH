---
title: "A simple explanation for the failure of change-of-variables adjustments"
author: "Jason Willwerscheid"
date: "11/14/2019"
output:
  workflowr::wflow_html:
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = TRUE, comment = "#>")
```

I've [argued](https://willwerscheid.github.io/scFLASH/pseudocount_redux.html#results:_elbo) that when fitting scRNA-seq data, the log likelihood of the EBMF model can be made infinitely large by taking the pseudocount $\alpha \to 0$. Here I give a simpler explanation for this phenomenon.

As usual, let $X$ be a $n \times p$ matrix of raw counts and let $Y$ be a $n \times p$ matrix of transformed counts
$$ Y = \log(X + \alpha). $$

As before, I begin with the observation that as $\alpha \to 0$, $Y$ is approximately binary, with $Y_{ij} = \log \alpha$ where $X_{ij} = 0$ and $Y_{ij} \approx 0$ where $X_{ij} > 0$.

Now, how does the (unadjusted) ELBO change as $\alpha \to 0$? Fix $\alpha$ to something small and consider the even smaller pseudocount $\alpha^c$, where $c > 1$. The transformed matrix is
$$ \tilde{Y} = \log(X + \alpha^c) \approx c Y $$
since $\tilde{Y}_{ij} = c \log \alpha$ where $X_{ij} = 0$ and $\tilde{Y}_{ij} \approx 0$ where $X_{ij} > 0$. That is, $c$ simply scales the transformed data matrix $Y$, which doesn't affect the KL-divergence between priors and posteriors but increases the data log likelihood by $np \log c$. (This can be calculated directly by noting that scaling the data by a factor of $c$ scales both the estimated residual standard deviations and expected squared residuals by a factor of $c$. And indeed, $np \log c$ is the adjustment that would be obtained using the change-of-variables formula with the scaling transformation.)

Thus the ELBO increases logarithmically as the logged pseudocount goes to $-\infty$.

But how does the ELBO adjustment obtained using the change-of-variables formula $\log |f'(x)|$ behave as $\alpha \to 0$? Fix $\alpha$ as above, but now consider $\alpha / k$ for some $k > 1$. The ELBO adjustment is:
$$ - \sum_{i, j} \log(X_{ij} + \alpha / k) + \sum_{i, j} \log(X_{ij} + \alpha) 
\approx \sum_{\{i, j: X_{ij} = 0\}} \log k = snp \log k, $$
where $s$ is the sparsity of $X$ (i.e., the proportion of counts that are nonzero).

Thus the ELBO adjustment increases logarithmically as the (non-logged) pseudocount goes to $-\infty$. In other words, even though the ELBO actually increases logarithmically as the logged pseudocount goes to $-\infty$, it is adjusted as if it were increasing linearly, which drives the adjusted ELBO to $\infty$.

There is, of course, a fundamental problem with using a change-of-variables adjustment with binary data: since the tranformation only really transforms two values, the derivative of the transformation is not defined. My argument shows, I think, that this problem can confound attempts to use transformations on sparse data as well. (I hesitate to generalize to discrete data, because it might be possible to obtain good results by requiring transformations to be smooth and monotonic.)
